"""
–ú–æ–¥—É–ª—å –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤ LLM —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º –≤—ã–±–æ—Ä–æ–º –º–æ–¥–µ–ª–µ–π v5.0

–î–∏–Ω–∞–º–∏—á–µ—Å–∫–∞—è –ø—Ä–∏–æ—Ä–∏—Ç–∏–∑–∞—Ü–∏—è –ø–æ —Ä–∞–∑–º–µ—Ä—É –º–æ–¥–µ–ª–∏:
- HEAVY –∑–∞–¥–∞—á–∏ ‚Üí –±–æ–ª—å—à–∏–µ –º–æ–¥–µ–ª–∏ –ø–µ—Ä–≤—ã–µ (—Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ —É–±—ã–≤–∞–Ω–∏—é —Ä–∞–∑–º–µ—Ä–∞)
- LIGHT –∑–∞–¥–∞—á–∏ ‚Üí –º–∞–ª–µ–Ω—å–∫–∏–µ –º–æ–¥–µ–ª–∏ –ø–µ—Ä–≤—ã–µ (—Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –≤–æ–∑—Ä–∞—Å—Ç–∞–Ω–∏—é —Ä–∞–∑–º–µ—Ä–∞)
- MEDIUM –∑–∞–¥–∞—á–∏ ‚Üí –ø–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é)

–ë–ï–ó –•–ê–†–î–ö–û–î–ê –º–æ–¥–µ–ª–µ–π - —Ä–∞–∑–º–µ—Ä –∏–∑–≤–ª–µ–∫–∞–µ—Ç—Å—è –∏–∑ –Ω–∞–∑–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–∏.
"""

import json
import logging
import os
import re
import time
import random
import requests
from abc import ABC, abstractmethod
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from enum import Enum
from threading import Lock
from typing import Dict, Any, Optional, Type, TypeVar, Union, List

from pydantic import BaseModel

logger = logging.getLogger(__name__)
T = TypeVar('T', bound=BaseModel)


# =============================================================================
# –ü–µ—Ä–µ—á–∏—Å–ª–µ–Ω–∏—è –∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
# =============================================================================

class LLMProviderType(str, Enum):
    """
    –¢–∏–ø—ã –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã—Ö LLM –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤.
    """
    OPENROUTER = "openrouter"
    GROQ = "groq"
    GOOGLE = "google"
    OLLAMA = "ollama"


class TaskType(str, Enum):
    """
    –¢–∏–ø—ã –∑–∞–¥–∞—á –¥–ª—è –≤—ã–±–æ—Ä–∞ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏.

    HEAVY: –°–ª–æ–∂–Ω—ã–µ –∑–∞–¥–∞—á–∏ ‚Üí –±–æ–ª—å—à–∏–µ –º–æ–¥–µ–ª–∏ –ø–µ—Ä–≤—ã–µ (–ø–æ —É–±—ã–≤–∞–Ω–∏—é —Ä–∞–∑–º–µ—Ä–∞)
    LIGHT: –ü—Ä–æ—Å—Ç—ã–µ –∑–∞–¥–∞—á–∏ ‚Üí –º–∞–ª–µ–Ω—å–∫–∏–µ –º–æ–¥–µ–ª–∏ –ø–µ—Ä–≤—ã–µ (–ø–æ –≤–æ–∑—Ä–∞—Å—Ç–∞–Ω–∏—é —Ä–∞–∑–º–µ—Ä–∞)
    MEDIUM: –°—Ä–µ–¥–Ω–∏–µ –∑–∞–¥–∞—á–∏ ‚Üí –ø–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é)
    """
    HEAVY = "heavy"
    MEDIUM = "medium"
    LIGHT = "light"


@dataclass
class LLMConfig:
    """
    –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è LLM –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞.
    """
    provider: LLMProviderType
    model: str
    temperature: float = 0.7
    max_tokens: int = 4096
    api_key: Optional[str] = None
    base_url: Optional[str] = None
    context_length: int = 131072

    OPENROUTER_DEFAULT_URL = "https://openrouter.ai/api/v1"
    GROQ_DEFAULT_URL = "https://api.groq.com/openai/v1"
    GOOGLE_DEFAULT_URL = "https://generativelanguage.googleapis.com/v1beta"
    OLLAMA_DEFAULT_URL = "http://ollama:11434"

    def __post_init__(self):
        if isinstance(self.provider, str):
            self.provider = LLMProviderType(self.provider.lower())

    def get_base_url(self) -> str:
        if self.base_url:
            return self.base_url

        defaults = {
            LLMProviderType.OPENROUTER: self.OPENROUTER_DEFAULT_URL,
            LLMProviderType.GROQ: self.GROQ_DEFAULT_URL,
            LLMProviderType.GOOGLE: self.GOOGLE_DEFAULT_URL,
            LLMProviderType.OLLAMA: self.OLLAMA_DEFAULT_URL,
        }
        return defaults.get(self.provider, "")

    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> "LLMConfig":
        try:
            provider = data.get("provider", "ollama")
            if isinstance(provider, str):
                provider = LLMProviderType(provider.lower())

            # –î–ª—è ollama: –µ—Å–ª–∏ model –Ω–µ –∑–∞–¥–∞–Ω —è–≤–Ω–æ, –±–µ—Ä—ë–º –∏–∑ env ‚Üí –¥–µ—Ñ–æ–ª—Ç
            default_model = (
                os.getenv("OLLAMA_MODEL", "qwen2.5:14b-instruct-q5_k_m")
                if provider == LLMProviderType.OLLAMA
                else "qwen2.5:14b-instruct-q5_k_m"
            )

            return cls(
                provider=provider,
                model=data.get("model") or default_model,
                temperature=data.get("temperature", 0.7),
                max_tokens=data.get("max_tokens", 4096),
                api_key=data.get("api_key"),
                base_url=data.get("base_url"),
                context_length=data.get("context_length", 131072),
            )
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è LLMConfig: {e}")
            return cls(
                provider=LLMProviderType.OLLAMA,
                model=os.getenv("OLLAMA_MODEL", "glm-4.7-flash:q4_K_M"),
            )


# =============================================================================
# –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π OpenRouter
# =============================================================================

@dataclass
class FreeModel:
    """
    –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –±–µ—Å–ø–ª–∞—Ç–Ω–æ–π –º–æ–¥–µ–ª–∏ OpenRouter.
    """
    id: str
    name: str
    context_length: int
    max_output: int
    capabilities: List[str] = field(default_factory=list)

    def __repr__(self) -> str:
        return f"FreeModel({self.id}, ctx={self.context_length})"


class OpenRouterModelDiscovery:
    """
    –î–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –±–µ—Å–ø–ª–∞—Ç–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π OpenRouter.

    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫—É –ø–æ —Ä–∞–∑–º–µ—Ä—É –º–æ–¥–µ–ª–∏ –¥–ª—è TaskType.
    """

    _instance = None
    _lock = Lock()

    API_URL = "https://openrouter.ai/api/v1/models"
    CACHE_TTL_SECONDS = 3600

    def __new__(cls):
        if cls._instance is None:
            with cls._lock:
                if cls._instance is None:
                    cls._instance = super().__new__(cls)
                    cls._instance._initialized = False
        return cls._instance

    def __init__(self):
        if getattr(self, '_initialized', False):
            return

        self._models: List[FreeModel] = []
        self._last_fetch: Optional[datetime] = None
        self._api_key = os.getenv("OPENROUTER_API_KEY")
        self._initialized = True
        logger.info("OpenRouterModelDiscovery –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")

    def get_model_size(self, model_id: str) -> int:
        """
        –ò–∑–≤–ª–µ—á—å —Ä–∞–∑–º–µ—Ä –º–æ–¥–µ–ª–∏ –∏–∑ –µ—ë ID (–≤ –º–∏–ª–ª–∏–∞—Ä–¥–∞—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤).

        –ü—Ä–∏–º–µ—Ä—ã:
            qwen/qwen3-235b ‚Üí 235
            meta-llama/llama-3.3-70b-instruct ‚Üí 70
            google/gemma-3-4b-it ‚Üí 4
            deepseek/deepseek-chat ‚Üí 100 (–¥–µ—Ñ–æ–ª—Ç –¥–ª—è –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö)

        Returns:
            –†–∞–∑–º–µ—Ä –≤ –º–∏–ª–ª–∏–∞—Ä–¥–∞—Ö (int)
        """
        model_lower = model_id.lower()

        # –ò—â–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã —Ä–∞–∑–º–µ—Ä–∞: 235b, 70b, 4b, 3.5b –∏ —Ç.–¥.
        patterns = [
            r'(\d+\.?\d*)b(?:-|:|$|_|\.)',  # 70b-, 235b:, 4b_, 70b.
            r'-(\d+\.?\d*)b',                # -70b
            r'(\d+\.?\d*)b-',                # 70b-
            r'[/-](\d+)b',                   # /70b –∏–ª–∏ -70b
        ]

        for pattern in patterns:
            match = re.search(pattern, model_lower)
            if match:
                try:
                    size = float(match.group(1))
                    return int(size)
                except:
                    pass

        # –ò–∑–≤–µ—Å—Ç–Ω—ã–µ –±–æ–ª—å—à–∏–µ –º–æ–¥–µ–ª–∏ –±–µ–∑ —Ä–∞–∑–º–µ—Ä–∞ –≤ –Ω–∞–∑–≤–∞–Ω–∏–∏
        big_models = {
            'deepseek-chat': 100,
            'deepseek-v3': 100,
            'gpt-4': 100,
            'claude': 100,
            'gemini-pro': 50,
            'gemini-2': 50,
            'gemini-flash': 30,
        }
        for name, size in big_models.items():
            if name in model_lower:
                return size

        # –î–µ—Ñ–æ–ª—Ç –¥–ª—è –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã—Ö
        return 20

    def get_models_sorted_by_size(
            self,
            ascending: bool = True,
            min_context: int = 4000
    ) -> List[FreeModel]:
        """
        –ü–æ–ª—É—á–∏—Ç—å –º–æ–¥–µ–ª–∏ –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–æ —Ä–∞–∑–º–µ—Ä—É.

        Args:
            ascending: True = –º–∞–ª–µ–Ω—å–∫–∏–µ –ø–µ—Ä–≤—ã–µ, False = –±–æ–ª—å—à–∏–µ –ø–µ—Ä–≤—ã–µ
            min_context: –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç

        Returns:
            –û—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π
        """
        models = self.get_free_models(min_context=min_context)

        if not models:
            return []

        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Ä–∞–∑–º–µ—Ä—É
        sorted_models = sorted(
            models,
            key=lambda m: self.get_model_size(m.id),
            reverse=not ascending
        )

        return sorted_models

    def get_free_models(
            self,
            min_context: int = 4000,
            force_refresh: bool = False
    ) -> List[FreeModel]:
        """
        –ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –±–µ—Å–ø–ª–∞—Ç–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π.
        """
        try:
            if not force_refresh and self._models and self._last_fetch:
                age = (datetime.now() - self._last_fetch).total_seconds()
                if age < self.CACHE_TTL_SECONDS:
                    logger.debug(f"–ò—Å–ø–æ–ª—å–∑—É–µ–º –∫—ç—à –º–æ–¥–µ–ª–µ–π: {len(self._models)} –º–æ–¥–µ–ª–µ–π")
                    return [m for m in self._models if m.context_length >= min_context]

            logger.info("–ó–∞–ø—Ä–æ—Å —Å–ø–∏—Å–∫–∞ –º–æ–¥–µ–ª–µ–π —Å OpenRouter API...")
            self._fetch_models()

            filtered = [m for m in self._models if m.context_length >= min_context]
            logger.info(f"–ü–æ–ª—É—á–µ–Ω–æ {len(self._models)} –º–æ–¥–µ–ª–µ–π, –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ: {len(filtered)}")

            return filtered

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π: {e}")
            return []

    def get_best_model(self, min_context: int = 4000) -> Optional[FreeModel]:
        """–ü–æ–ª—É—á–∏—Ç—å –ª—É—á—à—É—é –±–µ—Å–ø–ª–∞—Ç–Ω—É—é –º–æ–¥–µ–ª—å."""
        try:
            models = self.get_free_models(min_context=min_context)
            return models[0] if models else None
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏: {e}")
            return None

    def _fetch_models(self) -> None:
        """–ó–∞–ø—Ä–æ—Å–∏—Ç—å –º–æ–¥–µ–ª–∏ —Å API OpenRouter."""
        headers = {}
        if self._api_key:
            headers["Authorization"] = f"Bearer {self._api_key}"

        max_retries = 3
        retry_delay = 5

        for attempt in range(max_retries):
            try:
                logger.info(f"–ü–æ–ø—ã—Ç–∫–∞ {attempt + 1}/{max_retries} –∑–∞–ø—Ä–æ—Å–∞ –º–æ–¥–µ–ª–µ–π...")

                response = requests.get(
                    self.API_URL,
                    headers=headers,
                    timeout=60
                )

                if response.status_code != 200:
                    if response.status_code == 408 and attempt < max_retries - 1:
                        logger.warning(f"–¢–∞–π–º–∞—É—Ç, –ø–æ–≤—Ç–æ—Ä —á–µ—Ä–µ–∑ {retry_delay}—Å...")
                        time.sleep(retry_delay)
                        retry_delay *= 2
                        continue

                    logger.error(f"–û—à–∏–±–∫–∞ API: {response.status_code}")
                    if attempt == max_retries - 1:
                        self._use_fallback_models()
                    return

                data = response.json()
                models = []

                for item in data.get("data", []):
                    try:
                        pricing = item.get("pricing", {})
                        prompt_price = float(pricing.get("prompt", "1") or "1")
                        completion_price = float(pricing.get("completion", "1") or "1")

                        if prompt_price == 0 and completion_price == 0:
                            model = FreeModel(
                                id=item.get("id", ""),
                                name=item.get("name", ""),
                                context_length=item.get("context_length", 4096),
                                max_output=item.get("top_provider", {}).get("max_completion_tokens", 4096),
                                capabilities=self._extract_capabilities(item),
                            )
                            models.append(model)
                    except Exception as e:
                        logger.warning(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –º–æ–¥–µ–ª–∏ {item.get('id')}: {e}")

                models.sort(key=lambda m: (-m.context_length, m.name))

                self._models = models
                self._last_fetch = datetime.now()

                logger.info(f"–£—Å–ø–µ—à–Ω–æ –ø–æ–ª—É—á–µ–Ω–æ {len(models)} –º–æ–¥–µ–ª–µ–π")
                return

            except requests.exceptions.RequestException as e:
                if "timeout" in str(e).lower() and attempt < max_retries - 1:
                    logger.warning(f"–°–µ—Ç–µ–≤–∞—è –æ—à–∏–±–∫–∞, –ø–æ–≤—Ç–æ—Ä —á–µ—Ä–µ–∑ {retry_delay}—Å...")
                    time.sleep(retry_delay)
                    retry_delay *= 2
                    continue

                logger.error(f"–°–µ—Ç–µ–≤–∞—è –æ—à–∏–±–∫–∞: {e}")
                if attempt == max_retries - 1:
                    self._use_fallback_models()
                return
            except Exception as e:
                logger.error(f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞: {e}")
                if attempt == max_retries - 1:
                    self._use_fallback_models()
                return

        logger.error("–í—Å–µ –ø–æ–ø—ã—Ç–∫–∏ –∑–∞–ø—Ä–æ—Å–∞ –º–æ–¥–µ–ª–µ–π –ø—Ä–æ–≤–∞–ª–µ–Ω—ã")
        self._use_fallback_models()

    def _extract_capabilities(self, item: Dict) -> List[str]:
        """–ò–∑–≤–ª–µ—á—å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏ –∏–∑ –¥–∞–Ω–Ω—ã—Ö API."""
        try:
            caps = ["chat"]

            arch = item.get("architecture", {})
            modality = arch.get("modality", "")
            if "image" in modality.lower():
                caps.append("vision")

            params = item.get("supported_parameters", [])
            if params and ("tools" in params or "functions" in params):
                caps.append("function_calling")

            return caps
        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π: {e}")
            return ["chat"]

    def _use_fallback_models(self) -> None:
        """–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø—Ä–µ–¥–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π –ø—Ä–∏ –æ—à–∏–±–∫–µ API."""
        logger.warning("–ò—Å–ø–æ–ª—å–∑—É–µ–º —Ä–µ–∑–µ—Ä–≤–Ω—ã–π —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π")
        self._models = [
            FreeModel("google/gemini-2.0-flash-exp:free", "Gemini 2.0 Flash", 1048576, 8192, ["chat", "vision"]),
            FreeModel("meta-llama/llama-4-scout:free", "Llama 4 Scout", 524288, 16384, ["chat"]),
            FreeModel("meta-llama/llama-4-maverick:free", "Llama 4 Maverick", 131072, 16384, ["chat", "vision"]),
            FreeModel("deepseek/deepseek-chat-v3-0324:free", "DeepSeek Chat V3", 131072, 8192, ["chat"]),
            FreeModel("mistralai/mistral-small-3.1-24b-instruct:free", "Mistral Small 3.1", 131072, 8192, ["chat"]),
            FreeModel("qwen/qwen2.5-72b-instruct:free", "Qwen 2.5 72B", 131072, 8192, ["chat"]),
            FreeModel("meta-llama/llama-3.3-70b-instruct:free", "Llama 3.3 70B", 131072, 8192, ["chat"]),
            FreeModel("meta-llama/llama-3.1-8b-instruct:free", "Llama 3.1 8B", 131072, 4096, ["chat"]),
        ]
        self._last_fetch = datetime.now()


# =============================================================================
# –û—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è –º–æ–¥–µ–ª–µ–π
# =============================================================================

@dataclass
class ModelStatus:
    """–°—Ç–∞—Ç—É—Å –º–æ–¥–µ–ª–∏ –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è rate-limit."""
    model_id: str
    errors: int = 0
    cooldown_until: Optional[datetime] = None
    last_success: Optional[datetime] = None

    @property
    def is_available(self) -> bool:
        if self.cooldown_until is None:
            return True
        return datetime.now() >= self.cooldown_until

    def record_error(self) -> None:
        try:
            self.errors += 1
            cooldown = min(30 * (2 ** (self.errors - 1)), 600)
            jitter = cooldown * 0.2 * (random.random() * 2 - 1)
            cooldown += jitter

            self.cooldown_until = datetime.now() + timedelta(seconds=cooldown)
            logger.warning(f"–ú–æ–¥–µ–ª—å {self.model_id}: cooldown {cooldown:.0f}—Å (–æ—à–∏–±–æ–∫: {self.errors})")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–ø–∏—Å–∏ –æ—à–∏–±–∫–∏ –º–æ–¥–µ–ª–∏: {e}")

    def record_success(self) -> None:
        try:
            self.errors = 0
            self.cooldown_until = None
            self.last_success = datetime.now()
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–ø–∏—Å–∏ —É—Å–ø–µ—Ö–∞ –º–æ–¥–µ–ª–∏: {e}")


class ModelStatusTracker:
    """–°–∏–Ω–≥–ª—Ç–æ–Ω –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è —Å—Ç–∞—Ç—É—Å–∞ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π."""

    _instance = None
    _lock = Lock()

    def __new__(cls):
        if cls._instance is None:
            with cls._lock:
                if cls._instance is None:
                    cls._instance = super().__new__(cls)
                    cls._instance._statuses = {}
        return cls._instance

    def get(self, model_id: str) -> ModelStatus:
        if model_id not in self._statuses:
            self._statuses[model_id] = ModelStatus(model_id=model_id)
        return self._statuses[model_id]

    def is_available(self, model_id: str) -> bool:
        try:
            return self.get(model_id).is_available
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏ {model_id}: {e}")
            return False

    def record_error(self, model_id: str) -> None:
        try:
            self.get(model_id).record_error()
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–ø–∏—Å–∏ –æ—à–∏–±–∫–∏ –º–æ–¥–µ–ª–∏ {model_id}: {e}")

    def record_success(self, model_id: str) -> None:
        try:
            self.get(model_id).record_success()
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–ø–∏—Å–∏ —É—Å–ø–µ—Ö–∞ –º–æ–¥–µ–ª–∏ {model_id}: {e}")

    def get_available_models(self, model_ids: List[str]) -> List[str]:
        try:
            return [m for m in model_ids if self.is_available(m)]
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π: {e}")
            return []


# =============================================================================
# –ë–∞–∑–æ–≤—ã–π –ø—Ä–æ–≤–∞–π–¥–µ—Ä
# =============================================================================

class LLMProvider(ABC):
    """–ê–±—Å—Ç—Ä–∞–∫—Ç–Ω—ã–π –±–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å –¥–ª—è LLM –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤."""

    def __init__(self, config: Union[Dict[str, Any], LLMConfig]):
        try:
            if isinstance(config, dict):
                self.config = LLMConfig.from_dict(config)
            else:
                self.config = config

            self.provider_name = self.config.provider.value
            self.model = self.config.model
            self.api_key = self.config.api_key
            self.base_url = self.config.get_base_url()

            self._request_count = 0
            self._error_count = 0

            logger.info(f"–ü—Ä–æ–≤–∞–π–¥–µ—Ä {self.provider_name} –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞: {e}")
            raise

    @abstractmethod
    def generate(
            self,
            prompt: str,
            system_prompt: Optional[str] = None,
            temperature: Optional[float] = None,
            max_tokens: Optional[int] = None,
            **kwargs
    ) -> str:
        pass

    def generate_structured(
            self,
            prompt: str,
            output_schema: Type[T],
            system_prompt: Optional[str] = None,
            max_retries: int = 3,
            **kwargs
    ) -> T:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞."""
        last_error = None

        for attempt in range(max_retries):
            try:
                example_json = self._build_example_json(output_schema)

                json_prompt = f"""{prompt}

–í–ê–ñ–ù–û: –û—Ç–≤–µ—Ç—å –¢–û–õ–¨–ö–û –≤–∞–ª–∏–¥–Ω—ã–º JSON –æ–±—ä–µ–∫—Ç–æ–º. –ù–∏–∫–∞–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –¥–æ –∏–ª–∏ –ø–æ—Å–ª–µ JSON.
–ó–∞–ø–æ–ª–Ω–∏ –≤—Å–µ –ø–æ–ª—è —Å–≤–æ–∏–º–∏ —Ä–µ–∞–ª—å–Ω—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏ –ø–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º –∞–Ω–∞–ª–∏–∑–∞.

–§–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞ (–∑–∞–ø–æ–ª–Ω–∏ —Å–≤–æ–∏–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏):
{example_json}

–ù–∞—á–Ω–∏ –æ—Ç–≤–µ—Ç —Å {{ –∏ –∑–∞–∫–æ–Ω—á–∏ }}. –ë–µ–∑ markdown, –±–µ–∑ ```. –¢–æ–ª—å–∫–æ JSON."""

                temp = 0.1 if attempt >= max_retries - 1 else 0.2

                response = self.generate(
                    json_prompt,
                    system_prompt,
                    temperature=temp,
                    **kwargs
                )

                json_data = self._extract_json(response)
                if json_data:
                    json_data = self._clean_schema_artifacts(json_data, output_schema)
                    return output_schema(**json_data)

                last_error = ValueError("–ù–µ –Ω–∞–π–¥–µ–Ω –≤–∞–ª–∏–¥–Ω—ã–π JSON –≤ –æ—Ç–≤–µ—Ç–µ")

            except Exception as e:
                last_error = e
                logger.warning(f"–ü–æ–ø—ã—Ç–∫–∞ {attempt + 1} —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –≤—ã–≤–æ–¥–∞ –Ω–µ —É–¥–∞–ª–∞—Å—å: {e}")
                if attempt < max_retries - 1:
                    time.sleep(1)

        raise ValueError(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç: {last_error}")

    def _build_example_json(self, schema: Type[BaseModel]) -> str:
        """–ü–æ—Å—Ç—Ä–æ–∏—Ç—å JSON-–ø—Ä–∏–º–µ—Ä –Ω–∞ –æ—Å–Ω–æ–≤–µ Pydantic —Å—Ö–µ–º—ã."""
        try:
            try:
                fields = schema.model_fields
            except AttributeError:
                fields = schema.__fields__

            example = {}
            for field_name, field_info in fields.items():
                try:
                    annotation = field_info.annotation
                    description = field_info.description or ""
                except AttributeError:
                    annotation = field_info.outer_type_
                    description = field_info.field_info.description or ""

                example[field_name] = self._example_value_for_type(
                    field_name, annotation, description
                )

            return json.dumps(example, ensure_ascii=False, indent=2)
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –ø—Ä–∏–º–µ—Ä–∞ JSON: {e}")
            return "{}"

    def _example_value_for_type(self, name: str, annotation, description: str) -> Any:
        """–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–∏–º–µ—Ä –∑–Ω–∞—á–µ–Ω–∏—è –¥–ª—è –ø–æ–ª—è."""
        try:
            type_str = str(annotation).lower() if annotation else ""

            if annotation is int or "int" in type_str:
                if "score" in name or "–æ—Ü–µ–Ω–∫" in name.lower():
                    return 7
                return 5

            if annotation is float or "float" in type_str:
                return 0.8

            if annotation is bool or "bool" in type_str:
                return True

            if "list" in type_str:
                if "categor" in name or "–∫–∞—Ç–µ–≥" in name.lower():
                    return ["AI/ML", "DevOps"]
                if "tag" in name:
                    return ["python", "machine-learning"]
                return ["–ø—Ä–∏–º–µ—Ä1", "–ø—Ä–∏–º–µ—Ä2"]

            if annotation is str or "str" in type_str:
                if "reason" in name or "–ø—Ä–∏—á–∏–Ω" in name.lower():
                    return "–≤–∞—à–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ –∑–¥–µ—Å—å"
                if "audience" in name or "–∞—É–¥–∏—Ç–æ—Ä" in name.lower():
                    return "developers"
                if "categor" in name:
                    return "Technology"
                if description:
                    return f"<{description[:40]}>"
                return f"<–∑–∞–ø–æ–ª–Ω–∏—Ç–µ {name}>"

            return f"<{name}>"
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø—Ä–∏–º–µ—Ä–∞ –∑–Ω–∞—á–µ–Ω–∏—è: {e}")
            return "<–∑–Ω–∞—á–µ–Ω–∏–µ>"

    def _clean_schema_artifacts(self, data: Dict[str, Any], schema: Type[BaseModel]) -> Dict[str, Any]:
        """–û—á–∏—Å—Ç–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –æ—Ç –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤ JSON Schema."""
        try:
            try:
                expected_fields = set(schema.model_fields.keys())
            except AttributeError:
                expected_fields = set(schema.__fields__.keys())

            schema_meta_fields = {
                "description", "title", "type", "properties",
                "required", "$defs", "definitions", "additionalProperties"
            }

            data_keys = set(data.keys())
            is_schema_response = (
                    data_keys & schema_meta_fields and
                    not (data_keys & expected_fields)
            )

            if is_schema_response:
                if "properties" in data:
                    props = data["properties"]
                    extracted = {}
                    for key, val in props.items():
                        if key in expected_fields:
                            if isinstance(val, dict) and "default" in val:
                                extracted[key] = val["default"]
                            elif isinstance(val, dict) and "example" in val:
                                extracted[key] = val["example"]
                            else:
                                extracted[key] = val
                    if extracted:
                        logger.warning("–ú–æ–¥–µ–ª—å –≤–µ—Ä–Ω—É–ª–∞ JSON Schema, –∏–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ")
                        return extracted

                logger.warning("–ú–æ–¥–µ–ª—å –≤–µ—Ä–Ω—É–ª–∞ JSON Schema, –Ω–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –¥–∞–Ω–Ω—ã–µ")

            cleaned = {
                k: v for k, v in data.items()
                if k not in schema_meta_fields or k in expected_fields
            }

            if "content_structure" in cleaned and isinstance(cleaned["content_structure"], str):
                try:
                    if cleaned["content_structure"].strip().startswith("{"):
                        cleaned["content_structure"] = json.loads(cleaned["content_structure"])
                    else:
                        sections = cleaned["content_structure"].split(",")
                        cleaned["content_structure"] = {
                            "sections": [section.strip() for section in sections]
                        }
                except:
                    cleaned["content_structure"] = {"raw": cleaned["content_structure"]}

            return cleaned
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö: {e}")
            return data

    def _extract_json(self, text: str) -> Optional[Dict[str, Any]]:
        """–ò–∑–≤–ª–µ—á—å JSON –∏–∑ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –æ—Ç–≤–µ—Ç–∞."""
        if not text:
            return None

        text = text.strip()

        try:
            return json.loads(text)
        except json.JSONDecodeError:
            pass

        json_blocks = re.findall(r'```json\s*([\s\S]*?)\s*```', text, re.IGNORECASE)
        for block in json_blocks:
            try:
                return json.loads(block.strip())
            except json.JSONDecodeError:
                continue

        code_blocks = re.findall(r'```\s*([\s\S]*?)\s*```', text)
        for block in code_blocks:
            try:
                return json.loads(block.strip())
            except json.JSONDecodeError:
                continue

        start = text.find('{')
        if start != -1:
            brace_count = 0
            for i in range(start, len(text)):
                if text[i] == '{':
                    brace_count += 1
                elif text[i] == '}':
                    brace_count -= 1
                    if brace_count == 0:
                        try:
                            return json.loads(text[start:i + 1])
                        except json.JSONDecodeError:
                            break

        try:
            cleaned = re.sub(r'^[^{]*', '', text)
            cleaned = re.sub(r'}[^}]*$', '}', cleaned)
            return json.loads(cleaned)
        except json.JSONDecodeError:
            pass

        return None

    def get_metrics(self) -> Dict[str, Any]:
        return {
            "provider": self.provider_name,
            "model": self.model,
            "requests": self._request_count,
            "errors": self._error_count,
        }


# =============================================================================
# OpenRouter –ø—Ä–æ–≤–∞–π–¥–µ—Ä —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º fallback –∏ –ø—Ä–∏–æ—Ä–∏—Ç–∏–∑–∞—Ü–∏–µ–π –ø–æ –∑–∞–¥–∞—á–∞–º
# =============================================================================

class OpenRouterProvider(LLMProvider):
    """
    –ü—Ä–æ–≤–∞–π–¥–µ—Ä OpenRouter —Å –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–π –ø—Ä–∏–æ—Ä–∏—Ç–∏–∑–∞—Ü–∏–µ–π –º–æ–¥–µ–ª–µ–π.

    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç TaskType –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏:
    - HEAVY ‚Üí –±–æ–ª—å—à–∏–µ –º–æ–¥–µ–ª–∏ –ø–µ—Ä–≤—ã–µ
    - LIGHT ‚Üí –º–∞–ª–µ–Ω—å–∫–∏–µ –º–æ–¥–µ–ª–∏ –ø–µ—Ä–≤—ã–µ
    """

    def __init__(self, config: Union[Dict[str, Any], LLMConfig]):
        super().__init__(config)

        try:
            self.api_base = self.base_url or LLMConfig.OPENROUTER_DEFAULT_URL
            self.headers = {
                "Authorization": f"Bearer {self.api_key}",
                "Content-Type": "application/json",
                "HTTP-Referer": "https://github.com/news-aggregator",
                "X-Title": "News Aggregator Pro"
            }

            self._discovery = OpenRouterModelDiscovery()
            self._tracker = ModelStatusTracker()

            self._fallback_count = 0
            self._current_model = self.model

            logger.info(f"OpenRouter –ø—Ä–æ–≤–∞–π–¥–µ—Ä –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω —Å –º–æ–¥–µ–ª—å—é: {self.model}")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ OpenRouter –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞: {e}")
            raise

    def generate(
            self,
            prompt: str,
            system_prompt: Optional[str] = None,
            temperature: Optional[float] = None,
            max_tokens: Optional[int] = None,
            **kwargs
    ) -> str:
        """–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º fallback."""
        return self._generate_internal(
            prompt=prompt,
            system_prompt=system_prompt,
            temperature=temperature,
            max_tokens=max_tokens,
            task_type=None
        )

    def generate_for_task(
            self,
            prompt: str,
            task_type: TaskType,
            system_prompt: Optional[str] = None,
            temperature: Optional[float] = None,
            max_tokens: Optional[int] = None
    ) -> str:
        """
        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å —É—á—ë—Ç–æ–º —Ç–∏–ø–∞ –∑–∞–¥–∞—á–∏.

        Args:
            prompt: –ü—Ä–æ–º–ø—Ç
            task_type: –¢–∏–ø –∑–∞–¥–∞—á–∏ (HEAVY, MEDIUM, LIGHT)
            system_prompt: –°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç
            temperature: –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞
            max_tokens: –ú–∞–∫—Å. —Ç–æ–∫–µ–Ω–æ–≤

        Returns:
            –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
        """
        return self._generate_internal(
            prompt=prompt,
            system_prompt=system_prompt,
            temperature=temperature,
            max_tokens=max_tokens,
            task_type=task_type
        )

    def _generate_internal(
            self,
            prompt: str,
            system_prompt: Optional[str],
            temperature: Optional[float],
            max_tokens: Optional[int],
            task_type: Optional[TaskType]
    ) -> str:
        """–í–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π –º–µ—Ç–æ–¥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏."""
        self._request_count += 1

        try:
            temp = temperature or self.config.temperature
            tokens = max_tokens or self.config.max_tokens

            models_to_try = self._get_models_to_try(task_type=task_type)

            if not models_to_try:
                raise Exception("–ù–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π - –≤—Å–µ –≤ cooldown")

            task_name = task_type.value if task_type else "default"
            logger.info(f"üéØ [{task_name}] –ü–æ–ø—ã—Ç–∫–∞ —Å {len(models_to_try)} –º–æ–¥–µ–ª—è–º–∏")

            tried = []
            last_error = None
            base_delay = 1
            max_delay = 10

            for i, model_id in enumerate(models_to_try):
                if model_id in tried:
                    continue

                tried.append(model_id)

                try:
                    if i > 0:
                        delay = min(base_delay * (2 ** (i - 1)), max_delay)
                        logger.debug(f"–ó–∞–¥–µ—Ä–∂–∫–∞ {delay:.2f}—Å –ø–µ—Ä–µ–¥ –ø–æ–ø—ã—Ç–∫–æ–π {i + 1}")
                        time.sleep(delay)

                    result = self._make_request(
                        model_id, prompt, system_prompt, temp, tokens
                    )

                    self._tracker.record_success(model_id)
                    self._current_model = model_id

                    size = self._discovery.get_model_size(model_id)
                    logger.info(f"‚úÖ [{task_name}] –£—Å–ø–µ—Ö: {model_id} ({size}B)")
                    return result

                except Exception as e:
                    last_error = e
                    error_str = str(e)

                    if any(code in error_str for code in ["429", "402", "403"]):
                        self._tracker.record_error(model_id)
                        logger.warning(f"‚ö†Ô∏è {model_id} rate-limited")
                        continue

                    if any(code in error_str for code in ["500", "502", "503"]):
                        logger.warning(f"‚ö†Ô∏è {model_id} server error")
                        time.sleep(2)
                        continue

                    logger.warning(f"‚ö†Ô∏è {model_id}: {str(e)[:60]}")
                    continue

            self._error_count += 1
            raise Exception(f"–í—Å–µ –º–æ–¥–µ–ª–∏ –æ—Ç–∫–∞–∑–∞–ª–∏ [{task_name}]. –ü–æ—Å–ª–µ–¥–Ω—è—è: {last_error}")

        except Exception as e:
            self._error_count += 1
            logger.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
            raise

    def _get_models_to_try(self, task_type: Optional[TaskType] = None) -> List[str]:
        """
        –°–æ–±—Ä–∞—Ç—å —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—É –¥–ª—è –∑–∞–¥–∞—á–∏.

        Args:
            task_type: –¢–∏–ø –∑–∞–¥–∞—á–∏
                - HEAVY ‚Üí –±–æ–ª—å—à–∏–µ –º–æ–¥–µ–ª–∏ –ø–µ—Ä–≤—ã–µ
                - LIGHT ‚Üí –º–∞–ª–µ–Ω—å–∫–∏–µ –º–æ–¥–µ–ª–∏ –ø–µ—Ä–≤—ã–µ
                - MEDIUM/None ‚Üí –ø–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É (–¥–µ—Ñ–æ–ª—Ç)

        Returns:
            –°–ø–∏—Å–æ–∫ ID –º–æ–¥–µ–ª–µ–π –≤ –ø–æ—Ä—è–¥–∫–µ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞
        """
        try:
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–æ—Ä—è–¥–æ–∫ —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏
            if task_type == TaskType.HEAVY:
                ascending = False  # –ë–æ–ª—å—à–∏–µ –ø–µ—Ä–≤—ã–µ
                task_name = "HEAVY"
            elif task_type == TaskType.LIGHT:
                ascending = True   # –ú–∞–ª–µ–Ω—å–∫–∏–µ –ø–µ—Ä–≤—ã–µ
                task_name = "LIGHT"
            else:
                ascending = None   # –ü–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É
                task_name = "MEDIUM"

            # –ü–æ–ª—É—á–∞–µ–º –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏
            if ascending is not None:
                all_models = self._discovery.get_models_sorted_by_size(ascending=ascending)
            else:
                all_models = self._discovery.get_free_models()

            # –§–∏–ª—å—Ç—Ä—É–µ–º –¥–æ—Å—Ç—É–ø–Ω—ã–µ (–Ω–µ –≤ cooldown)
            models = []
            for m in all_models:
                if self._tracker.is_available(m.id):
                    models.append(m.id)

            # –õ–æ–≥–∏—Ä—É–µ–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç
            logger.info(f"üìã [{task_name}] –ú–æ–¥–µ–ª–∏ –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—É ({len(models)} –¥–æ—Å—Ç—É–ø–Ω–æ):")
            for i, model_id in enumerate(models[:5]):
                size = self._discovery.get_model_size(model_id)
                logger.info(f"   {i+1}. {model_id} ({size}B)")
            if len(models) > 5:
                logger.info(f"   ... –∏ –µ—â—ë {len(models) - 5}")

            return models

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å–ø–∏—Å–∫–∞ –º–æ–¥–µ–ª–µ–π: {e}")
            return [self.model]

    def _make_request(
            self,
            model: str,
            prompt: str,
            system_prompt: Optional[str],
            temperature: float,
            max_tokens: int
    ) -> str:
        """–í—ã–ø–æ–ª–Ω–∏—Ç—å HTTP –∑–∞–ø—Ä–æ—Å –∫ API."""
        try:
            messages = []
            if system_prompt:
                messages.append({"role": "system", "content": system_prompt})
            messages.append({"role": "user", "content": prompt})

            data = {
                "model": model,
                "messages": messages,
                "temperature": temperature,
                "max_tokens": max_tokens,
            }

            response = requests.post(
                f"{self.api_base}/chat/completions",
                headers=self.headers,
                json=data,
                timeout=180
            )

            if response.status_code != 200:
                error_msg = f"OpenRouter {response.status_code}: {response.text[:200]}"
                logger.error(error_msg)
                raise Exception(error_msg)

            result = response.json()
            if "error" in result:
                error_msg = f"–û—à–∏–±–∫–∞ OpenRouter: {result['error']}"
                logger.error(error_msg)
                raise Exception(error_msg)

            return result["choices"][0]["message"]["content"]

        except requests.exceptions.Timeout:
            raise Exception(f"–¢–∞–π–º–∞—É—Ç –∑–∞–ø—Ä–æ—Å–∞ –∫ –º–æ–¥–µ–ª–∏ {model}")
        except requests.exceptions.RequestException as e:
            raise Exception(f"–°–µ—Ç–µ–≤–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ –∫ {model}: {e}")
        except Exception as e:
            raise

    def get_metrics(self) -> Dict[str, Any]:
        try:
            metrics = super().get_metrics()
            metrics.update({
                "current_model": self._current_model,
                "fallbacks": self._fallback_count,
            })
            return metrics
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫: {e}")
            return {}

    def get_available_models(self) -> List[str]:
        try:
            free_models = self._discovery.get_free_models()
            return [m.id for m in free_models if self._tracker.is_available(m.id)]
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π: {e}")
            return []

    def print_models(self, task_type: Optional[TaskType] = None) -> None:
        """–í—ã–≤–µ—Å—Ç–∏ —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π —Å –∏—Ö —Å—Ç–∞—Ç—É—Å–æ–º."""
        try:
            if task_type == TaskType.HEAVY:
                models = self._discovery.get_models_sorted_by_size(ascending=False)
                title = "HEAVY (–±–æ–ª—å—à–∏–µ –ø–µ—Ä–≤—ã–µ)"
            elif task_type == TaskType.LIGHT:
                models = self._discovery.get_models_sorted_by_size(ascending=True)
                title = "LIGHT (–º–∞–ª–µ–Ω—å–∫–∏–µ –ø–µ—Ä–≤—ã–µ)"
            else:
                models = self._discovery.get_free_models()
                title = "–ø–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É"

            print(f"\n{'=' * 70}")
            print(f"üÜì –ú–û–î–ï–õ–ò [{title}] ({len(models)} –¥–æ—Å—Ç—É–ø–Ω–æ)")
            print(f"{'=' * 70}")

            for i, m in enumerate(models[:15], 1):
                status = "‚úì" if self._tracker.is_available(m.id) else "‚è≥"
                size = self._discovery.get_model_size(m.id)
                ctx = f"{m.context_length // 1000}k"
                caps = ""
                if "vision" in m.capabilities:
                    caps += "üëÅ"
                if "function_calling" in m.capabilities:
                    caps += "üîß"
                print(f"{i:2}. {status} {m.id:<50} {size:>3}B ctx={ctx} {caps}")

            print(f"{'=' * 70}\n")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –≤—ã–≤–æ–¥–∞ –º–æ–¥–µ–ª–µ–π: {e}")


# =============================================================================
# –û—Å—Ç–∞–ª—å–Ω—ã–µ –ø—Ä–æ–≤–∞–π–¥–µ—Ä—ã
# =============================================================================

class GroqProvider(LLMProvider):
    """–ü—Ä–æ–≤–∞–π–¥–µ—Ä Groq —Å OpenAI-—Å–æ–≤–º–µ—Å—Ç–∏–º—ã–º API."""

    def __init__(self, config: Union[Dict[str, Any], LLMConfig]):
        super().__init__(config)
        try:
            self.api_base = self.base_url or LLMConfig.GROQ_DEFAULT_URL
            self.headers = {
                "Authorization": f"Bearer {self.api_key}",
                "Content-Type": "application/json"
            }
            logger.info("Groq –ø—Ä–æ–≤–∞–π–¥–µ—Ä –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ Groq –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞: {e}")
            raise

    def generate(
            self,
            prompt: str,
            system_prompt: Optional[str] = None,
            temperature: Optional[float] = None,
            max_tokens: Optional[int] = None,
            **kwargs
    ) -> str:
        try:
            self._request_count += 1

            messages = []
            if system_prompt:
                messages.append({"role": "system", "content": system_prompt})
            messages.append({"role": "user", "content": prompt})

            data = {
                "model": self.model,
                "messages": messages,
                "temperature": temperature or self.config.temperature,
                "max_tokens": max_tokens or self.config.max_tokens,
            }

            response = requests.post(
                f"{self.api_base}/chat/completions",
                headers=self.headers,
                json=data,
                timeout=60
            )

            if response.status_code != 200:
                error_msg = f"–û—à–∏–±–∫–∞ Groq: {response.status_code} {response.text}"
                logger.error(error_msg)
                self._error_count += 1
                raise Exception(error_msg)

            return response.json()["choices"][0]["message"]["content"]

        except Exception as e:
            self._error_count += 1
            logger.error(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ Groq: {e}")
            raise


class GoogleProvider(LLMProvider):
    """–ü—Ä–æ–≤–∞–π–¥–µ—Ä Google Gemini —Å –Ω–∞—Ç–∏–≤–Ω—ã–º API."""

    def __init__(self, config: Union[Dict[str, Any], LLMConfig]):
        super().__init__(config)
        try:
            self.api_base = self.base_url or LLMConfig.GOOGLE_DEFAULT_URL
            logger.info("Google –ø—Ä–æ–≤–∞–π–¥–µ—Ä –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ Google –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞: {e}")
            raise

    def generate(
            self,
            prompt: str,
            system_prompt: Optional[str] = None,
            temperature: Optional[float] = None,
            max_tokens: Optional[int] = None,
            **kwargs
    ) -> str:
        try:
            self._request_count += 1

            url = f"{self.api_base}/models/{self.model}:generateContent?key={self.api_key}"

            contents = []
            if system_prompt:
                contents.append({
                    "role": "user",
                    "parts": [{"text": f"System: {system_prompt}"}]
                })
                contents.append({
                    "role": "model",
                    "parts": [{"text": "Understood."}]
                })
            contents.append({
                "role": "user",
                "parts": [{"text": prompt}]
            })

            data = {
                "contents": contents,
                "generationConfig": {
                    "temperature": temperature or self.config.temperature,
                    "maxOutputTokens": max_tokens or self.config.max_tokens,
                }
            }

            response = requests.post(url, json=data, timeout=120)

            if response.status_code != 200:
                error_msg = f"–û—à–∏–±–∫–∞ Google: {response.status_code} {response.text}"
                logger.error(error_msg)
                self._error_count += 1
                raise Exception(error_msg)

            result = response.json()
            if "candidates" not in result:
                raise Exception(f"Google –Ω–µ –≤–µ—Ä–Ω—É–ª –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤: {result}")

            return result["candidates"][0]["content"]["parts"][0]["text"]

        except Exception as e:
            self._error_count += 1
            logger.error(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ Google: {e}")
            raise


class OllamaProvider(LLMProvider):
    """–ü—Ä–æ–≤–∞–π–¥–µ—Ä Ollama –¥–ª—è –ª–æ–∫–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π."""

    def __init__(self, config: Union[Dict[str, Any], LLMConfig]):
        super().__init__(config)
        try:
            self.api_base = self.base_url or LLMConfig.OLLAMA_DEFAULT_URL
            # –ú–æ–¥–µ–ª—å —É–∂–µ –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ –∑–∞–ø–æ–ª–Ω–µ–Ω–∞ —á–µ—Ä–µ–∑ LLMConfig/ModelsConfig.
            # Fallback –Ω–∞ env —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ config.model –ø—É—Å—Ç.
            self.model = self.config.model or os.getenv("OLLAMA_MODEL", "glm-4.7-flash:q4_K_M")
            logger.info(f"Ollama –ø—Ä–æ–≤–∞–π–¥–µ—Ä –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω —Å –º–æ–¥–µ–ª—å—é: {self.model}")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ Ollama –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞: {e}")
            raise

    def generate(
            self,
            prompt: str,
            system_prompt: Optional[str] = None,
            temperature: Optional[float] = None,
            max_tokens: Optional[int] = None,
            **kwargs
    ) -> str:
        try:
            self._request_count += 1

            full_prompt = f"{system_prompt}\n\n{prompt}" if system_prompt else prompt

            data = {
                "model": self.model,
                "prompt": full_prompt,
                "options": {
                    "temperature": temperature or self.config.temperature,
                    "num_predict": max_tokens or self.config.max_tokens,
                },
                "stream": False
            }

            response = requests.post(
                f"{self.api_base}/api/generate",
                json=data,
                timeout=3600
            )

            if response.status_code != 200:
                error_msg = f"–û—à–∏–±–∫–∞ Ollama: {response.status_code} {response.text}"
                logger.error(error_msg)
                self._error_count += 1
                raise Exception(error_msg)

            return response.json()["response"]

        except Exception as e:
            self._error_count += 1
            logger.error(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ Ollama: {e}")
            raise


# =============================================================================
# –§–∞–±—Ä–∏–∫–∞ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤
# =============================================================================

class LLMProviderFactory:
    """–§–∞–±—Ä–∏–∫–∞ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è LLM –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤."""

    _providers = {
        "groq": GroqProvider,
        "openrouter": OpenRouterProvider,
        "google": GoogleProvider,
        "ollama": OllamaProvider,
    }

    @classmethod
    def create(cls, config: Union[Dict[str, Any], LLMConfig]) -> LLMProvider:
        try:
            if isinstance(config, dict):
                config = LLMConfig.from_dict(config)

            provider_name = config.provider.value

            if provider_name not in cls._providers:
                raise ValueError(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –ø—Ä–æ–≤–∞–π–¥–µ—Ä: {provider_name}")

            return cls._providers[provider_name](config)
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞: {e}")
            raise

    @classmethod
    def create_auto(
            cls,
            provider: str = "openrouter",
            min_context: int = 4000,
            max_tokens: int = 4096,
            temperature: float = 0.7
    ) -> LLMProvider:
        try:
            if provider == "openrouter":
                discovery = OpenRouterModelDiscovery()
                best_model = discovery.get_best_model(min_context=min_context)

                if best_model:
                    model_id = best_model.id
                    logger.info(f"–ê–≤—Ç–æ-–≤—ã–±—Ä–∞–Ω–∞ –º–æ–¥–µ–ª—å: {model_id} (–∫–æ–Ω—Ç–µ–∫—Å—Ç: {best_model.context_length})")
                else:
                    model_id = "meta-llama/llama-3.1-8b-instruct:free"
                    logger.warning(f"–ú–æ–¥–µ–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã, –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback: {model_id}")

                config = LLMConfig(
                    provider=LLMProviderType.OPENROUTER,
                    model=model_id,
                    max_tokens=max_tokens,
                    temperature=temperature,
                    api_key=os.getenv("OPENROUTER_API_KEY"),
                    context_length=best_model.context_length if best_model else 131072,
                )
            else:
                # –ú–æ–¥–µ–ª—å –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç—Å—è —á–µ—Ä–µ–∑ env ‚Üí —Ä–∞–∑—É–º–Ω—ã–π –¥–µ—Ñ–æ–ª—Ç (–±–µ–∑ —Ö–∞—Ä–¥–∫–æ–¥–∞ qwen)
                env_models = {
                    "groq": os.getenv("GROQ_MODEL", "llama-3.1-8b-instant"),
                    "google": os.getenv("GOOGLE_MODEL", "gemini-1.5-flash"),
                    "ollama": os.getenv("OLLAMA_MODEL", "glm-4.7-flash:q4_K_M"),
                }

                config = LLMConfig(
                    provider=LLMProviderType(provider),
                    model=env_models.get(provider, "glm-4.7-flash:q4_K_M"),
                    max_tokens=max_tokens,
                    temperature=temperature,
                    api_key=os.getenv(f"{provider.upper()}_API_KEY"),
                )

            return cls.create(config)
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –∞–≤—Ç–æ-–ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞: {e}")
            raise

    @classmethod
    def available_providers(cls) -> List[str]:
        return list(cls._providers.keys())


# =============================================================================
# –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏
# =============================================================================

def get_free_models(min_context: int = 4000) -> List[FreeModel]:
    """–ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –±–µ—Å–ø–ª–∞—Ç–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π OpenRouter."""
    try:
        discovery = OpenRouterModelDiscovery()
        return discovery.get_free_models(min_context=min_context)
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –±–µ—Å–ø–ª–∞—Ç–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π: {e}")
        return []


def print_free_models(task_type: Optional[TaskType] = None) -> None:
    """–í—ã–≤–µ—Å—Ç–∏ —Å–ø–∏—Å–æ–∫ –±–µ—Å–ø–ª–∞—Ç–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π OpenRouter."""
    try:
        discovery = OpenRouterModelDiscovery()

        if task_type == TaskType.HEAVY:
            models = discovery.get_models_sorted_by_size(ascending=False)
            title = "HEAVY - –±–æ–ª—å—à–∏–µ –ø–µ—Ä–≤—ã–µ"
        elif task_type == TaskType.LIGHT:
            models = discovery.get_models_sorted_by_size(ascending=True)
            title = "LIGHT - –º–∞–ª–µ–Ω—å–∫–∏–µ –ø–µ—Ä–≤—ã–µ"
        else:
            models = discovery.get_free_models()
            title = "–ø–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É"

        print(f"\n{'=' * 70}")
        print(f"üÜì –ë–ï–°–ü–õ–ê–¢–ù–´–ï –ú–û–î–ï–õ–ò [{title}] ({len(models)} –Ω–∞–π–¥–µ–Ω–æ)")
        print(f"{'=' * 70}")

        for i, m in enumerate(models[:20], 1):
            size = discovery.get_model_size(m.id)
            ctx = f"{m.context_length // 1000}k" if m.context_length >= 1000 else str(m.context_length)
            caps = ""
            if "vision" in m.capabilities:
                caps += "üëÅ"
            if "function_calling" in m.capabilities:
                caps += "üîß"
            print(f"{i:2}. {m.id:<55} {size:>3}B ctx={ctx:<6} {caps}")

        print(f"{'=' * 70}\n")
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –≤—ã–≤–æ–¥–∞ –º–æ–¥–µ–ª–µ–π: {e}")


# =============================================================================
# CLI –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
# =============================================================================

if __name__ == "__main__":
    import sys

    try:
        print("\n=== HEAVY (–±–æ–ª—å—à–∏–µ –º–æ–¥–µ–ª–∏ –ø–µ—Ä–≤—ã–µ) ===")
        print_free_models(TaskType.HEAVY)

        print("\n=== LIGHT (–º–∞–ª–µ–Ω—å–∫–∏–µ –º–æ–¥–µ–ª–∏ –ø–µ—Ä–≤—ã–µ) ===")
        print_free_models(TaskType.LIGHT)

        if len(sys.argv) > 1 and sys.argv[1] == "--test":
            print("\nüß™ –¢–µ—Å—Ç–∏—Ä—É–µ–º...")

            try:
                provider = LLMProviderFactory.create_auto(min_context=8000)
                print(f"–í—ã–±—Ä–∞–Ω–∞: {provider.model}")

                response = provider.generate("–°–∫–∞–∂–∏ '–ü—Ä–∏–≤–µ—Ç –º–∏—Ä' —Ä–æ–≤–Ω–æ –¥–≤—É–º—è —Å–ª–æ–≤–∞–º–∏.")
                print(f"–û—Ç–≤–µ—Ç: {response}")
            except Exception as e:
                print(f"–û—à–∏–±–∫–∞: {e}")
    except Exception as e:
        logger.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")