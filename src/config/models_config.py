# -*- coding: utf-8 -*-
# =============================================================================
# –ü—É—Ç—å: src/config/models_config.py
# =============================================================================
"""
–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –ø—Ä–æ—Ñ–∏–ª–µ–π –∏ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤.

–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç:
- –ù–µ—Å–∫–æ–ª—å–∫–æ –ø—Ä–æ—Ñ–∏–ª–µ–π (balanced, fast, cloud_balanced –∏ –¥—Ä.)
- –ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ —á–µ—Ä–µ–∑ env-–ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
- YAML –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
- –î–µ—Ñ–æ–ª—Ç–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è

–ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è:
- LLM_PROVIDER: –ü–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –ø—Ä–æ–≤–∞–π–¥–µ—Ä (ollama/openrouter)
- LLM_PROFILE: –ü–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –ø—Ä–æ—Ñ–∏–ª—å
- OPENROUTER_API_KEY: API –∫–ª—é—á –¥–ª—è OpenRouter
- OLLAMA_BASE_URL: –ö–∞—Å—Ç–æ–º–Ω—ã–π URL Ollama
"""

import os
import yaml
from typing import Dict, Any, Optional
from dataclasses import dataclass, field
from enum import Enum

from src.infrastructure.ai.llm_provider import LLMConfig, LLMProviderType


class AgentType(str, Enum):
    """–¢–∏–ø—ã AI –∞–≥–µ–Ω—Ç–æ–≤."""
    CLASSIFIER = "classifier"
    RELEVANCE = "relevance"
    SUMMARIZER = "summarizer"
    REWRITER = "rewriter"
    STYLE_NORMALIZER = "style_normalizer"
    QUALITY_VALIDATOR = "quality_validator"


@dataclass
class AgentConfig:
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –æ–¥–Ω–æ–≥–æ –∞–≥–µ–Ω—Ç–∞."""
    model: str
    temperature: float
    max_tokens: int
    provider: LLMProviderType = LLMProviderType.OLLAMA

    def to_llm_config(
            self,
            base_url: Optional[str] = None,
            api_key: Optional[str] = None
    ) -> LLMConfig:
        """–ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å –≤ LLMConfig."""
        return LLMConfig(
            provider=self.provider,
            model=self.model,
            temperature=self.temperature,
            max_tokens=self.max_tokens,
            base_url=base_url,
            api_key=api_key
        )


@dataclass
class ProfileConfig:
    """–ü—Ä–æ—Ñ–∏–ª—å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ —Å–æ –≤—Å–µ–º–∏ –∞–≥–µ–Ω—Ç–∞–º–∏."""
    name: str
    provider: LLMProviderType
    agents: Dict[AgentType, AgentConfig] = field(default_factory=dict)


class ModelsConfig:
    """
    –ì–ª–∞–≤–Ω—ã–π –∫–ª–∞—Å—Å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –º–æ–¥–µ–ª–µ–π.

    –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç –∑–∞–≥—Ä—É–∑–∫–∏:
    1. –ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è (–≤—ã—Å—à–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç)
    2. YAML —Ñ–∞–π–ª –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
    3. –í—Å—Ç—Ä–æ–µ–Ω–Ω—ã–µ –¥–µ—Ñ–æ–ª—Ç—ã (–Ω–∏–∑—à–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç)
    """

    # –í—Å—Ç—Ä–æ–µ–Ω–Ω—ã–µ –ø—Ä–æ—Ñ–∏–ª–∏
    DEFAULT_PROFILES = {
        "balanced": {
            "provider": "ollama",
            "agents": {
                "classifier": {"model": "qwen2.5:14b-instruct-q5_k_m", "temperature": 0.3, "max_tokens": 100},
                "relevance": {"model": "qwen2.5:14b-instruct-q5_k_m", "temperature": 0.4, "max_tokens": 300},
                "summarizer": {"model": "qwen2.5:14b-instruct-q5_k_m", "temperature": 0.5, "max_tokens": 300},
                "rewriter": {"model": "qwen2.5:14b-instruct-q5_k_m", "temperature": 0.6, "max_tokens": 200},
                "style_normalizer": {"model": "qwen2.5:14b-instruct-q5_k_m", "temperature": 0.3, "max_tokens": 8000},
                "quality_validator": {"model": "qwen2.5:14b-instruct-q5_k_m", "temperature": 0.2, "max_tokens": 500},
            }
        },
        "fast": {
            "provider": "ollama",
            "agents": {
                "classifier": {"model": "mistral:latest", "temperature": 0.3, "max_tokens": 100},
                "relevance": {"model": "mistral:latest", "temperature": 0.4, "max_tokens": 300},
                "summarizer": {"model": "mistral:latest", "temperature": 0.5, "max_tokens": 300},
                "rewriter": {"model": "mistral:latest", "temperature": 0.6, "max_tokens": 200},
                "style_normalizer": {"model": "qwen2.5:7b", "temperature": 0.3, "max_tokens": 4000},
                "quality_validator": {"model": "mistral:latest", "temperature": 0.2, "max_tokens": 500},
            }
        },
        # =========================================================================
        # –ë–ï–°–ü–õ–ê–¢–ù–´–ô –ü–†–û–§–ò–õ–¨ - OpenRouter —Å LiquidAI (FREE!)
        # =========================================================================
        "free_openrouter": {
            "provider": "openrouter",
            "agents": {
                "classifier": {"model": "liquid/lfm-2.5-1.2b-instruct", "temperature": 0.3, "max_tokens": 100},
                "relevance": {"model": "liquid/lfm-2.5-1.2b-instruct", "temperature": 0.4, "max_tokens": 300},
                "summarizer": {"model": "liquid/lfm-2.5-1.2b-instruct", "temperature": 0.5, "max_tokens": 400},
                "rewriter": {"model": "liquid/lfm-2.5-1.2b-instruct", "temperature": 0.6, "max_tokens": 300},
                "style_normalizer": {"model": "liquid/lfm-2.5-1.2b-instruct", "temperature": 0.3, "max_tokens": 4000},
                "quality_validator": {"model": "liquid/lfm-2.5-1.2b-instruct", "temperature": 0.2, "max_tokens": 500},
            }
        },
        "cloud_balanced": {
            "provider": "openrouter",
            "agents": {
                "classifier": {"model": "gpt-4o-mini", "temperature": 0.3, "max_tokens": 100},
                "relevance": {"model": "gpt-4o-mini", "temperature": 0.4, "max_tokens": 300},
                "summarizer": {"model": "gpt-4o-mini", "temperature": 0.5, "max_tokens": 300},
                "rewriter": {"model": "gpt-4o-mini", "temperature": 0.6, "max_tokens": 200},
                "style_normalizer": {"model": "gpt-4o-mini", "temperature": 0.3, "max_tokens": 4000},
                "quality_validator": {"model": "gpt-4o-mini", "temperature": 0.2, "max_tokens": 500},
            }
        },
        "cloud_quality": {
            "provider": "openrouter",
            "agents": {
                "classifier": {"model": "gpt-4o", "temperature": 0.2, "max_tokens": 100},
                "relevance": {"model": "gpt-4o", "temperature": 0.3, "max_tokens": 500},
                "summarizer": {"model": "claude-3.5-sonnet", "temperature": 0.4, "max_tokens": 500},
                "rewriter": {"model": "claude-3.5-sonnet", "temperature": 0.5, "max_tokens": 300},
                "style_normalizer": {"model": "claude-3.5-sonnet", "temperature": 0.2, "max_tokens": 8000},
                "quality_validator": {"model": "gpt-4o", "temperature": 0.1, "max_tokens": 500},
            }
        },
    }

    def __init__(self, config_path: str = "config/models.yaml"):
        self.config_path = config_path
        self._raw_config = self._load_config()

        # –ü–æ–ª—É—á–∏—Ç—å –∞–∫—Ç–∏–≤–Ω—ã–π –ø—Ä–æ—Ñ–∏–ª—å –∏–∑ env –∏–ª–∏ –∫–æ–Ω—Ñ–∏–≥–∞
        self.active_profile = os.getenv(
            "LLM_PROFILE",
            self._raw_config.get("active_profile", "balanced")
        )

        # –ü–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞ –∏–∑ env
        self._provider_override = os.getenv("LLM_PROVIDER")

        # API –∫–ª—é—á–∏ –∏ URL –∏–∑ env
        self.openrouter_api_key = os.getenv("OPENROUTER_API_KEY")
        self.ollama_base_url = os.getenv("OLLAMA_BASE_URL", "http://ollama:11434")

    def _load_config(self) -> Dict[str, Any]:
        """–ó–∞–≥—Ä—É–∑–∏—Ç—å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –∏–∑ YAML —Ñ–∞–π–ª–∞."""
        try:
            with open(self.config_path, 'r', encoding='utf-8') as f:
                return yaml.safe_load(f) or {}
        except FileNotFoundError:  # –ò–°–ü–†–ê–í–õ–ï–ù–û: –±—ã–ª–æ "FileNotFound–û—à–∏–±–∫–∞"
            print(f"‚ö†Ô∏è  –ö–æ–Ω—Ñ–∏–≥ {self.config_path} –Ω–µ –Ω–∞–π–¥–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º –¥–µ—Ñ–æ–ª—Ç—ã")
            return {}
        except Exception as e:
            print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è –∫–æ–Ω—Ñ–∏–≥–∞: {e}, –∏—Å–ø–æ–ª—å–∑—É–µ–º –¥–µ—Ñ–æ–ª—Ç—ã")
            return {}

    def get_profile(self) -> ProfileConfig:
        """–ü–æ–ª—É—á–∏—Ç—å —Ç–µ–∫—É—â–∏–π –ø—Ä–æ—Ñ–∏–ª—å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏."""
        # –ü–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –∏–∑ YAML
        profiles = self._raw_config.get("profiles", {})

        if self.active_profile in profiles:
            profile_data = profiles[self.active_profile]
        elif self.active_profile in self.DEFAULT_PROFILES:
            profile_data = self.DEFAULT_PROFILES[self.active_profile]
        else:
            print(f"‚ö†Ô∏è  –ü—Ä–æ—Ñ–∏–ª—å '{self.active_profile}' –Ω–µ –Ω–∞–π–¥–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º 'balanced'")
            profile_data = self.DEFAULT_PROFILES["balanced"]

        # –†–∞—Å–ø–∞—Ä—Å–∏—Ç—å –ø—Ä–æ–≤–∞–π–¥–µ—Ä
        provider_str = self._provider_override or profile_data.get("provider", "ollama")
        provider = LLMProviderType(provider_str.lower())

        # –†–∞—Å–ø–∞—Ä—Å–∏—Ç—å –∞–≥–µ–Ω—Ç–æ–≤
        agents = {}
        for agent_name, agent_data in profile_data.get("agents", {}).items():  # –ò–°–ü–†–ê–í–õ–ï–ù–û: –±—ã–ª–æ "—ç–ª–µ–º–µ–Ω—Ç–æ–≤()"
            agent_type = AgentType(agent_name)
            agents[agent_type] = AgentConfig(
                model=agent_data["model"],
                temperature=agent_data["temperature"],
                max_tokens=agent_data["max_tokens"],
                provider=provider
            )

        return ProfileConfig(
            name=self.active_profile,
            provider=provider,
            agents=agents
        )

    def get_agent_config(self, agent_name: str) -> AgentConfig:
        """–ü–æ–ª—É—á–∏—Ç—å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –∞–≥–µ–Ω—Ç–∞."""
        profile = self.get_profile()
        agent_type = AgentType(agent_name)

        if agent_type not in profile.agents:
            # –í–µ—Ä–Ω—É—Ç—å –¥–µ—Ñ–æ–ª—Ç–Ω—ã–π –∫–æ–Ω—Ñ–∏–≥
            return AgentConfig(
                model="mistral:latest",
                temperature=0.5,
                max_tokens=500,
                provider=profile.provider
            )

        return profile.agents[agent_type]

    def get_llm_config(self, agent_name: str) -> LLMConfig:
        """–ü–æ–ª—É—á–∏—Ç—å LLMConfig –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –∞–≥–µ–Ω—Ç–∞."""
        agent_config = self.get_agent_config(agent_name)
        profile = self.get_profile()

        # –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å base_url –∏ api_key –ø–æ –ø—Ä–æ–≤–∞–π–¥–µ—Ä—É
        if profile.provider == LLMProviderType.OPENROUTER:
            base_url = LLMConfig.OPENROUTER_DEFAULT_URL
            api_key = self.openrouter_api_key
        else:
            base_url = self.ollama_base_url
            api_key = None

        return agent_config.to_llm_config(base_url=base_url, api_key=api_key)

    def get_provider(self) -> LLMProviderType:
        """–ü–æ–ª—É—á–∏—Ç—å —Ç–µ–∫—É—â–∏–π —Ç–∏–ø –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞."""
        return self.get_profile().provider

    # –ú–µ—Ç–æ–¥—ã –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
    def get_model(self, agent_name: str) -> str:
        """–ü–æ–ª—É—á–∏—Ç—å –º–æ–¥–µ–ª—å –¥–ª—è –∞–≥–µ–Ω—Ç–∞."""
        return self.get_agent_config(agent_name).model

    def get_temperature(self, agent_name: str) -> float:
        """–ü–æ–ª—É—á–∏—Ç—å —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—É –¥–ª—è –∞–≥–µ–Ω—Ç–∞."""
        return self.get_agent_config(agent_name).temperature

    def get_max_tokens(self, agent_name: str) -> int:
        """–ü–æ–ª—É—á–∏—Ç—å max_tokens –¥–ª—è –∞–≥–µ–Ω—Ç–∞."""
        return self.get_agent_config(agent_name).max_tokens

    def print_config(self):
        """–í—ã–≤–µ—Å—Ç–∏ —Ç–µ–∫—É—â—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é."""
        profile = self.get_profile()

        print(f"\n{'=' * 70}")
        print(f"üìã –ê–ö–¢–ò–í–ù–´–ô –ü–†–û–§–ò–õ–¨: {profile.name}")
        print(f"üîå –ü–†–û–í–ê–ô–î–ï–†: {profile.provider.value}")
        print(f"{'=' * 70}\n")

        for agent_type, agent_config in profile.agents.items():  # –ò–°–ü–†–ê–í–õ–ï–ù–û: –±—ã–ª–æ "—ç–ª–µ–º–µ–Ω—Ç–æ–≤()"
            print(
                f"ü§ñ {agent_type.value:20} ‚Üí {agent_config.model:30} "
                f"(T={agent_config.temperature}, tokens={agent_config.max_tokens})"
            )

        print(f"\n{'=' * 70}\n")

    @classmethod
    def get_available_profiles(cls) -> list[str]:
        """–ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã—Ö –ø—Ä–æ—Ñ–∏–ª–µ–π."""
        return list(cls.DEFAULT_PROFILES.keys())


# –ì–ª–æ–±–∞–ª—å–Ω—ã–π –∏–Ω—Å—Ç–∞–Ω—Å –¥–ª—è —É–¥–æ–±—Å—Ç–≤–∞
_config_instance: Optional[ModelsConfig] = None


def get_models_config(config_path: str = "config/models.yaml") -> ModelsConfig:
    """–ü–æ–ª—É—á–∏—Ç—å –∏–ª–∏ —Å–æ–∑–¥–∞—Ç—å –≥–ª–æ–±–∞–ª—å–Ω—ã–π –∏–Ω—Å—Ç–∞–Ω—Å ModelsConfig."""
    global _config_instance
    if _config_instance is None:
        _config_instance = ModelsConfig(config_path)
    return _config_instance