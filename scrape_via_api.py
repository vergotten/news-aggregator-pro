#!/usr/bin/env python3
"""
ĞŸĞ°Ñ€ÑĞµÑ€ Ñ‡ĞµÑ€ĞµĞ· API - Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ‘Ğ•Ğ— Ğ¿Ñ€ÑĞ¼Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´ĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ñ Ğº Ğ‘Ğ”.
Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ: python scrape_via_api.py [limit] [hubs]
"""

import sys
import asyncio
import aiohttp
from bs4 import BeautifulSoup
from datetime import datetime


async def scrape_habr(limit: int = 10, hubs: str = ""):
    """ĞŸĞ°Ñ€ÑĞ¸Ğ½Ğ³ Habr Ğ¸ Ğ¾Ñ‚Ğ¿Ñ€Ğ°Ğ²ĞºĞ° Ñ‡ĞµÑ€ĞµĞ· API."""
    
    print(f"\nğŸš€ Ğ—Ğ°Ğ¿ÑƒÑĞº Habr Ğ¿Ğ°Ñ€ÑĞµÑ€Ğ° Ñ‡ĞµÑ€ĞµĞ· API")
    print(f"   Ğ›Ğ¸Ğ¼Ğ¸Ñ‚: {limit}")
    print(f"   Ğ¥Ğ°Ğ±Ñ‹: {hubs if hubs else 'Ğ²ÑĞµ'}\n")
    
    api_url = "http://localhost:8000/api/v1/articles/"
    habr_url = "https://habr.com/ru/articles/"
    
    stats = {'scraped': 0, 'saved': 0, 'duplicates': 0, 'errors': 0}
    
    # ĞŸĞ°Ñ€ÑĞ¸Ğ¼ Habr
    async with aiohttp.ClientSession() as session:
        # ĞŸĞ¾Ğ»ÑƒÑ‡Ğ°ĞµĞ¼ HTML
        async with session.get(habr_url) as response:
            html = await response.text()
        
        soup = BeautifulSoup(html, 'html.parser')
        articles = soup.find_all('article', class_='tm-articles-list__item', limit=limit)
        
        stats['scraped'] = len(articles)
        
        # ĞÑ‚Ğ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼ ĞºĞ°Ğ¶Ğ´ÑƒÑ ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ² API
        for article_card in articles:
            try:
                # ĞŸĞ°Ñ€ÑĞ¸Ğ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ
                title_elem = article_card.find('h2', class_='tm-title')
                if not title_elem:
                    continue
                
                title_link = title_elem.find('a')
                title = title_link.text.strip()
                url = "https://habr.com" + title_link['href']
                
                # ĞĞ²Ñ‚Ğ¾Ñ€
                author_elem = article_card.find('a', class_='tm-user-info__username')
                author = author_elem.text.strip() if author_elem else None
                
                # ĞšĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚
                content_elem = article_card.find('div', class_='article-formatted-body')
                content = content_elem.text.strip()[:500] if content_elem else ""
                
                # Ğ¥Ğ°Ğ±Ñ‹
                hub_elems = article_card.find_all('a', class_='tm-publication-hub__link')
                article_hubs = [h.text.strip() for h in hub_elems]
                
                # ĞÑ‚Ğ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğ² API
                payload = {
                    "title": title,
                    "content": content,
                    "url": url,
                    "source": "habr",
                    "author": author,
                    "tags": article_hubs,
                    "hubs": article_hubs
                }
                
                async with session.post(api_url, json=payload) as resp:
                    if resp.status == 201:
                        stats['saved'] += 1
                        print(f"âœ“ {title[:50]}...")
                    elif resp.status == 400:
                        error = await resp.text()
                        if 'already exists' in error.lower():
                            stats['duplicates'] += 1
                            print(f"âŠ— Ğ”ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ‚: {title[:40]}...")
                        else:
                            stats['errors'] += 1
                            print(f"âœ— ĞÑˆĞ¸Ğ±ĞºĞ°: {title[:40]}...")
                    else:
                        stats['errors'] += 1
                        print(f"âœ— HTTP {resp.status}: {title[:40]}...")
                        
            except Exception as e:
                stats['errors'] += 1
                print(f"âœ— ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ğ°Ñ€ÑĞ¸Ğ½Ğ³Ğ°: {e}")
    
    # Ğ˜Ñ‚Ğ¾Ğ³Ğ¸
    print(f"\nâœ… Ğ“Ğ¾Ñ‚Ğ¾Ğ²Ğ¾!")
    print(f"   Ğ¡Ğ¾Ğ±Ñ€Ğ°Ğ½Ğ¾: {stats['scraped']}")
    print(f"   Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¾: {stats['saved']}")
    print(f"   Ğ”ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ‚Ğ¾Ğ²: {stats['duplicates']}")
    print(f"   ĞÑˆĞ¸Ğ±Ğ¾Ğº: {stats['errors']}\n")


if __name__ == '__main__':
    limit = int(sys.argv[1]) if len(sys.argv) > 1 else 10
    hubs = sys.argv[2] if len(sys.argv) > 2 else ""
    
    asyncio.run(scrape_habr(limit, hubs))
